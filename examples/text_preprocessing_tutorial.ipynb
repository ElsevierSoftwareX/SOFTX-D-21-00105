{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCL Tutorial\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Preparation](#Preparation)\n",
    "3. [Filtering on language](#Filtering-on-language)\n",
    "4. [Filtering on Jaccard similarity](#Filtering-on-Jaccard-similarity)\n",
    "5. [Filtering on perplexity score](#Filtering-on-perplexity-score)\n",
    "6. [Outliers filtering](#Outliers-filtering)\n",
    "7. [Plots for outlier detection](#Plots-for-outlier-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates how to preprocess text to clean it up for the modeling stage. Modeling can include classification models, NER (Named Entity Recognition) models, spell checking models, text summarization and generation, as well as prediction models. Usually, this analysis is performed manually and is very time-consuming. The TextCL package helps to identify and filter out (1) sentences in languages other than the target language, (2) linguistically unconnected and/or corrupted sentences, and (3) duplicate sentences.\n",
    "\n",
    "Another feature of the package is to identify and filter out outliers from the text scope. As outliers, we consider texts that don't contextually belong to the main topic of the text. It's important to be able to identify these anomalies without having labeled data, so we can have a general algorithm for unstructured texts and find out the scope blocks.\n",
    "\n",
    "In this tutorial, we will work with the [BBC data set](http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) and additional manually generated sentences to demonstrate the package's functionality. Overall, the package can be used with any text data set loaded as a [Pandas data frame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "The package contains the following functions for text cleaning:\n",
    "\n",
    "1. Filtering on language\n",
    "2. Filtering on Jaccard similarity\n",
    "3. Filtering on perplexity score\n",
    "4. Outliers filtering\n",
    "\n",
    "The first three functions work at the sentence level for each text in the scope. In turn, the outlier filtering function works at the level of the full text. The latter implements 3 different outlier detection algorithms: [TONMF](https://arxiv.org/pdf/1701.01325.pdf), [RPCA](https://github.com/dganguli/robust-pca), and [SVD](https://api.semanticscholar.org/CorpusID:123532178), with `l2` normalisation by default (can be changed to the `l1`, `l2`, or `max` via the `norm` parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load package and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textcl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#set up seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the input data from the [modified BBC dataset](https://github.com/alinapetukhova/textcl/blob/master/examples/prepared_bbc_dataset.csv) bundled with TextCL. First, load the text data. The sample file is located in the `examples` folder, next to this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num texts in the data set: 21\n"
     ]
    }
   ],
   "source": [
    "SOURCE_FILE_PATH = 'prepared_bbc_dataset.csv'\n",
    "\n",
    "input_texts_df = pd.read_csv(SOURCE_FILE_PATH).reset_index()\n",
    "print(\"Num texts in the data set: {}\".format(len(input_texts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we'll use a subset of the BBC News data set containing 5 topics (business, entertainment, politics, sport, tech) with manually inserted texts to demonstrate the capabilities of the package. Here's how the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Profits slide at India's Dr Reddy  Profits at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>Liberian economy starts to grow  The Liberian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>business</td>\n",
       "      <td>Uluslararası Para Fonu (IMF), Liberya ekonomis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Singer Ian Brown 'in gig arrest'  Former Stone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Blue beat U2 to top France honour  Irish band ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Housewives lift Channel 4 ratings  The debut o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Домохозяйки подняли рейтинги канала 4 Дебют ам...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Housewives Channel 4 reytinglerini yükseltti A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>politics</td>\n",
       "      <td>Observers to monitor UK election  Ministers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>politics</td>\n",
       "      <td>Lib Dems highlight problem debt  People vulner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>politics</td>\n",
       "      <td>Minister defends hunting ban law  The law bann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>sport</td>\n",
       "      <td>Legendary Dutch boss Michels dies  Legendary D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>sport</td>\n",
       "      <td>Connors boost for British tennis  Former world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>sport</td>\n",
       "      <td>Sociedad set to rescue Mladenovic  Rangers are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>tech</td>\n",
       "      <td>Mobile games come of age  The BBC News website...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>tech</td>\n",
       "      <td>PlayStation 3 processor unveiled  The Cell pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>tech</td>\n",
       "      <td>PC photo printers challenge printed pictures c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>tech</td>\n",
       "      <td>PC photo printers challenge pros  Home printed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>tech</td>\n",
       "      <td>processor come pros 43 t6 43 Table data 342 5 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>tech</td>\n",
       "      <td>Janice Dean currently serves as senior meteoro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index     topic_name                                               text\n",
       "0       0       business  WorldCom bosses' $54m payout  Ten former direc...\n",
       "1       1       business  Profits slide at India's Dr Reddy  Profits at ...\n",
       "2       2       business  Liberian economy starts to grow  The Liberian ...\n",
       "3       3       business  Uluslararası Para Fonu (IMF), Liberya ekonomis...\n",
       "4       4  entertainment  Singer Ian Brown 'in gig arrest'  Former Stone...\n",
       "5       5  entertainment  Blue beat U2 to top France honour  Irish band ...\n",
       "6       6  entertainment  Housewives lift Channel 4 ratings  The debut o...\n",
       "7       7  entertainment  Домохозяйки подняли рейтинги канала 4 Дебют ам...\n",
       "8       8  entertainment  Housewives Channel 4 reytinglerini yükseltti A...\n",
       "9       9       politics  Observers to monitor UK election  Ministers wi...\n",
       "10     10       politics  Lib Dems highlight problem debt  People vulner...\n",
       "11     11       politics  Minister defends hunting ban law  The law bann...\n",
       "12     12          sport  Legendary Dutch boss Michels dies  Legendary D...\n",
       "13     13          sport  Connors boost for British tennis  Former world...\n",
       "14     14          sport  Sociedad set to rescue Mladenovic  Rangers are...\n",
       "15     15           tech  Mobile games come of age  The BBC News website...\n",
       "16     16           tech  PlayStation 3 processor unveiled  The Cell pro...\n",
       "17     17           tech  PC photo printers challenge printed pictures c...\n",
       "18     18           tech  PC photo printers challenge pros  Home printed...\n",
       "19     19           tech  processor come pros 43 t6 43 Table data 342 5 ...\n",
       "20     20           tech  Janice Dean currently serves as senior meteoro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split texts into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to process/filter sentences from the data set separately, we first need to split our texts into sentences as rows in a Pandas data frame. The [`split_into_sentences()`](https://alinapetukhova.github.io/textcl/docs/preprocessing.html#textcl.preprocessing.split_into_sentences) function is used for this purpose.\n",
    "\n",
    "Notice the data loaded in the previous section has a column named `text`. By default, the `split_into_sentences()` function expects to find texts in this column. However, an alternative name for this column can be specified in the function's `text_col` parameter.\n",
    "\n",
    "Splitting the data set texts into sentences is done as follows, using the `split_into_sentences()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences before filtering: 319\n"
     ]
    }
   ],
   "source": [
    "split_input_texts_df = textcl.split_into_sentences(input_texts_df)\n",
    "print(\"Num sentences before filtering: {}\".format(len(split_input_texts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting text data set into sentences, the number of rows increased from 21 to 319. Let's review them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "      <td>James Wareham, a lawyer representing one of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "      <td>The remaining $36m will be paid by the directo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "      <td>But, a spokesman for the prosecutor, New York ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "      <td>Corporate governance experts said that if the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index topic_name                                               text  \\\n",
       "0      0   business  WorldCom bosses' $54m payout  Ten former direc...   \n",
       "1      0   business  WorldCom bosses' $54m payout  Ten former direc...   \n",
       "2      0   business  WorldCom bosses' $54m payout  Ten former direc...   \n",
       "3      0   business  WorldCom bosses' $54m payout  Ten former direc...   \n",
       "4      0   business  WorldCom bosses' $54m payout  Ten former direc...   \n",
       "\n",
       "                                            sentence  \n",
       "0  WorldCom bosses' $54m payout  Ten former direc...  \n",
       "1   James Wareham, a lawyer representing one of t...  \n",
       "2  The remaining $36m will be paid by the directo...  \n",
       "3  But, a spokesman for the prosecutor, New York ...  \n",
       "4   Corporate governance experts said that if the...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_texts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in this example, the `split_into_sentences()` function places sentences in the `sentence` column by default. This can be changed using the function's `sentence_col` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering on language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the language filtering function works with manually inserted texts in Russian and Turkish languages to the initial set.\n",
    "\n",
    "To do this we'll use the [`language_filtering()`](https://alinapetukhova.github.io/textcl/docs/preprocessing.html#textcl.preprocessing.language_filtering) function, which filters sentences by language. The input to this function should be a Pandas data frame (with `sentence` column), a threshold value, and a target language. Language score is the threshold used for filtering, with the default value of 0.99. The function makes use of the `detect_language` function from the [langdetect](https://pypi.org/project/langdetect/) package, which returns probabilities of a text belonging to a certain language. All sentences below this threshold will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences after language filtering: 281\n"
     ]
    }
   ],
   "source": [
    "split_input_texts_df = textcl.language_filtering(split_input_texts_df, threshold=0.99, language='en')\n",
    "print(\"Num sentences after language filtering: {}\".format(len(split_input_texts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with sentences was reduced from 319 to 281. Join sentences to the initial texts to review the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Profits slide at India's Dr Reddy  Profits at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liberian economy starts to grow  The Liberian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer Ian Brown 'in gig arrest'  Former Stone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Blue beat U2 to top France honour  Irish band ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Housewives lift Channel 4 ratings  The debut o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>Observers to monitor UK election  Ministers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Lib Dems highlight problem debt  People vulner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Minister defends hunting ban law  The law bann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>Legendary Dutch boss Michels dies  Legendary D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>Connors boost for British tennis  Former world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>Sociedad set to rescue Mladenovic  Rangers are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>Mobile games come of age  The BBC News website...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>PlayStation 3 processor unveiled  The Cell pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>PC photo printers challenge printed pictures c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>PC photo printers challenge pros  Home printed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>data clear additional 78.0 long-term 43 those)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>Janice Dean currently serves as senior meteoro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           sentence\n",
       "0       0  WorldCom bosses' $54m payout  Ten former direc...\n",
       "1       1  Profits slide at India's Dr Reddy  Profits at ...\n",
       "2       2  Liberian economy starts to grow  The Liberian ...\n",
       "3       4  Singer Ian Brown 'in gig arrest'  Former Stone...\n",
       "4       5  Blue beat U2 to top France honour  Irish band ...\n",
       "5       6  Housewives lift Channel 4 ratings  The debut o...\n",
       "6       9  Observers to monitor UK election  Ministers wi...\n",
       "7      10  Lib Dems highlight problem debt  People vulner...\n",
       "8      11  Minister defends hunting ban law  The law bann...\n",
       "9      12  Legendary Dutch boss Michels dies  Legendary D...\n",
       "10     13  Connors boost for British tennis  Former world...\n",
       "11     14  Sociedad set to rescue Mladenovic  Rangers are...\n",
       "12     15  Mobile games come of age  The BBC News website...\n",
       "13     16  PlayStation 3 processor unveiled  The Cell pro...\n",
       "14     17  PC photo printers challenge printed pictures c...\n",
       "15     18  PC photo printers challenge pros  Home printed...\n",
       "16     19     data clear additional 78.0 long-term 43 those)\n",
       "17     20  Janice Dean currently serves as senior meteoro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcl.join_sentences_by_label(split_input_texts_df, label_col = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, texts with index 3 (Turkish), 7 (Russian) and 8 (Turkish) were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering on Jaccard similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function [`jaccard_sim_filtering()`](https://alinapetukhova.github.io/textcl/docs/preprocessing.html#textcl.preprocessing.jaccard_sim_filtering) is used to filter sentences by [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index). It represents each sentence as an array of tokens and finds the intersection between each pair of arrays / sentences. Using the intersection, the similarity score is calculated; if it's above the specified `threshold`, a sentence will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences after Jaccard sim filtering: 258\n"
     ]
    }
   ],
   "source": [
    "split_input_texts_df = textcl.jaccard_sim_filtering(split_input_texts_df, threshold=0.8)\n",
    "print(\"Num sentences after Jaccard sim filtering: {}\".format(len(split_input_texts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with sentences was reduced from 281 to 258. Join sentences to the initial texts to review the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>WorldCom bosses' $54m payout  Ten former direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Profits slide at India's Dr Reddy  Profits at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Liberian economy starts to grow  The Liberian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Singer Ian Brown 'in gig arrest'  Former Stone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Blue beat U2 to top France honour  Irish band ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Housewives lift Channel 4 ratings  The debut o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>Observers to monitor UK election  Ministers wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>Lib Dems highlight problem debt  People vulner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>Minister defends hunting ban law  The law bann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>Legendary Dutch boss Michels dies  Legendary D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>Connors boost for British tennis  Former world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>Sociedad set to rescue Mladenovic  Rangers are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>Mobile games come of age  The BBC News website...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>PlayStation 3 processor unveiled  The Cell pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>18</td>\n",
       "      <td>PC photo printers challenge pros  Home printed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19</td>\n",
       "      <td>data clear additional 78.0 long-term 43 those)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20</td>\n",
       "      <td>Janice Dean currently serves as senior meteoro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           sentence\n",
       "0       0  WorldCom bosses' $54m payout  Ten former direc...\n",
       "1       1  Profits slide at India's Dr Reddy  Profits at ...\n",
       "2       2  Liberian economy starts to grow  The Liberian ...\n",
       "3       4  Singer Ian Brown 'in gig arrest'  Former Stone...\n",
       "4       5  Blue beat U2 to top France honour  Irish band ...\n",
       "5       6  Housewives lift Channel 4 ratings  The debut o...\n",
       "6       9  Observers to monitor UK election  Ministers wi...\n",
       "7      10  Lib Dems highlight problem debt  People vulner...\n",
       "8      11  Minister defends hunting ban law  The law bann...\n",
       "9      12  Legendary Dutch boss Michels dies  Legendary D...\n",
       "10     13  Connors boost for British tennis  Former world...\n",
       "11     14  Sociedad set to rescue Mladenovic  Rangers are...\n",
       "12     15  Mobile games come of age  The BBC News website...\n",
       "13     16  PlayStation 3 processor unveiled  The Cell pro...\n",
       "14     18  PC photo printers challenge pros  Home printed...\n",
       "15     19     data clear additional 78.0 long-term 43 those)\n",
       "16     20  Janice Dean currently serves as senior meteoro..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textcl.join_sentences_by_label(split_input_texts_df, label_col = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text with $id=17$ was removed as it was a partial duplicate of text with $id=18$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering on perplexity score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function [`perplexity_filtering()`](https://alinapetukhova.github.io/textcl/docs/preprocessing.html#textcl.preprocessing.perplexity_filtering) is used to filter sentences by perplexity, i.e., when sentences are linguistically incorrect and/or unconnected with the remaining text. \n",
    "\n",
    "In general, **perplexity** is a measurement of how well a probability distribution or probability model predicts a sample. In the case of the text data, we will be checking the probability of the next word to be in the given sentence. A low perplexity indicates the probability distribution is good at predicting the word.\n",
    "\n",
    "The first step creates contextual tokens to capture latent syntactic-semantic information provided by the [pytorch_pretrained_bert](https://pypi.org/project/pytorch-pretrained-bert/) package with pretrained `openai-gpt` tokenizer and using GPT as a language model with `OpenAIGPTLMHeadModel`. Perplexity is calculated as `exp(loss)` (where **loss** is the language modeling loss of a particular token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "split_input_texts_df = textcl.perplexity_filtering(split_input_texts_df, threshold=1000)\n",
    "print(\"Num sentences after perplexity filtering: {}\".format(len(split_input_texts_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows with sentences was reduced from 258 to the 246. Join sentences to the initial texts to review the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcl.join_sentences_by_label(split_input_texts_df, label_col = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text with $id=19$ was removed because sentence *\"data clear additional 78.0 long-term 43 those)\"* is not linguistically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is uniquely challenging to outlier detection because of its sparsity and high-dimensional nature. In this package we will use Non-Negative Matrix Factorization (NMF) to detect the text topics in an unsupervised fashion, so the model can be trained and be able to detect outliers without labeling of topics. \n",
    "\n",
    "The way it works is that NMF decomposes high-dimensional vectors into a lower-dimensional representation. These lower-dimensional vectors are non-negative, which also means their coefficients are non-negative. With this approach, we can see the fact that NMF is similar to probabilistic latent semantic indexing (pLSI) and latent Dirichlet allocation (LDA) generative models. From the original matrix, $A$, NMF yields two matrices, $W$ and $H$. The former represents the topics detected in the text data set, while the latter contains the weights for those topics. In other words, $A$ is the documents by words matrix, $H$ is the articles by topics matrix and $W$ is the topics by words matrix.\n",
    "\n",
    "The [`outlier_detection()`](https://alinapetukhova.github.io/textcl/docs/outliers_detection.html#textcl.outliers_detection.outlier_detection) function is used to detect outliers in a list of sentences based on the contextual information using unsupervised methods. Text embeddings are created as a bag of words as an input for the implemented algorithms. The main input parameters for this function are the Pandas data frame containing the texts, the method to use for outlier detection ([TONMF](https://arxiv.org/pdf/1701.01325.pdf) by default), and the type of norm (`l2` by default) to normalize the obtained matrix and detecting the unrelated texts.\n",
    "\n",
    "To test this, we'll first join sentences to the text after filtering and selecting the \"tech\" category. This category has a manually inserted outlier ($id=20$) with a person profile instead of text on the subject of \"tech\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_texts = split_input_texts_df[[\"index\", \"text\", \"topic_name\"]].drop_duplicates()\n",
    "joined_texts = joined_texts[joined_texts.topic_name == 'tech']\n",
    "joined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the `outlier_detection()` function with the [RPCA](https://github.com/dganguli/robust-pca) algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_texts, _ = textcl.outlier_detection(joined_texts, method='rpca', Z_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of `joined_texts` are now as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text with $id=20$ was removed because it describes a person profile instead of tech news.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots for outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we'll visualize the outliers for the BBC data set obtained with the different algorithms. First load additional libraries required for results visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the [NLTK](https://www.nltk.org/) library to filter stop words from the initial dataset. Let's import this library and download the stop words list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data from the BBC dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how the package handles outlier detection tasks, we present a simplified example from a real world data set to show how skewed the typical values of the corresponding column $z(x)$ may be in real scenarios. To build plots for the outlier detection functions we'll need to load the [full BBC dataset](http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip) and apply initial basic text cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download from http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip and unpack to this folder:\n",
    "dataset_path = \"./datasets/bbc\"\n",
    "\n",
    "bbc_dataset = pd.DataFrame([], columns = ['class_name', 'text'])\n",
    "\n",
    "# Create a list of topics from the folder names based on the data set initial structure\n",
    "list_topic_folders = os.listdir(\"{}/\".format(dataset_path))\n",
    "\n",
    "# Get documents per each topic from the corresponding folder\n",
    "for topic_folder in list_topic_folders:\n",
    "    \n",
    "    # Skip ReadMe.txt file\n",
    "    if \"txt\" not in topic_folder.lower():\n",
    "        list_of_files = os.listdir(\"{}/{}\".format(dataset_path, topic_folder))\n",
    "        \n",
    "        # add to the Pandas data frame each text document in the folder\n",
    "        for file in list_of_files:\n",
    "            if file.find(\".txt\") != -1 and file.find(\"ipynb\") == -1:\n",
    "                with open(\"{}/{}/{}\".format(dataset_path, topic_folder, file), 'rb') as f:\n",
    "                    text = f.read()\n",
    "                text = text.decode('windows-1252').replace('\\n', ' ')\n",
    "                bbc_dataset = bbc_dataset.append(pd.DataFrame([[topic_folder, text]], columns = ['class_name', 'text']))\n",
    "\n",
    "# reset index in the final data frame to get identifier column for each document\n",
    "bbc_dataset = bbc_dataset.reset_index(drop=True)\n",
    "bbc_dataset.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set contains 2225 texts from the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in bbc_dataset['class_name'].unique():\n",
    "    print(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data for \"business\" and \"politics\" classes to form the core of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_and_pol = bbc_dataset[(bbc_dataset['class_name'] == \"business\") | (bbc_dataset['class_name'] == \"politics\")]\n",
    "df_bus_and_pol_texts = pd.DataFrame(list(bus_and_pol.text.values), columns=['text'])\n",
    "df_bus_and_pol_texts['y_true'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 50 random outliers documents from \"tech\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_for_outliers = bbc_dataset[bbc_dataset['class_name'] == \"tech\"]\n",
    "\n",
    "text_for_outliers = text_for_outliers.iloc[random.sample(range(0, len(text_for_outliers)), 50)]\n",
    "df_outliers = pd.DataFrame(list(text_for_outliers.text.values), columns=['text'])\n",
    "df_outliers['y_true'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add outliers documents to the core data set and shuffle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_bus_and_pol_texts, df_outliers])\n",
    "df_test = df_test.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df_test['text'] = df_test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert texts to a bag of words using sklearn's [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bag_of_words = vectorizer.fit_transform(df_test['text']).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `CountVectorizer` is used as an example. We could have used another tokenizer or word embedding.\n",
    "\n",
    "In the following subsections we'll plot the outlier detection results using the three methods included with TextCL, namely [TONMF](https://arxiv.org/pdf/1701.01325.pdf), [RPCA](https://github.com/dganguli/robust-pca), and [SVD](https://api.semanticscholar.org/CorpusID:123532178)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TONMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`tonmf()`](https://alinapetukhova.github.io/textcl/docs/outliers_detection.html#textcl.outliers_detection.tonmf) function uses the TONMF algorithm to obtain the outlier matrix. The solution is based on the non-negative matrix factorization with the extension of the block coordinate descent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_matrix,_,_,_ = textcl.tonmf(bag_of_words, k=10, alpha=10, beta=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize with l2-normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, y_pred = preprocessing.normalize(outlier_matrix, axis = 1, norm = 'l2', return_norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ROC curve to plot the results of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 8))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(list(df_test['y_true'].values), y_pred, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "f.savefig(\"BBC dataset TONMF ROC-curve. K=3, alpha = 1, beta = 0.5.pdf\", bbox_inches='tight')\n",
    "plt.title('BBC dataset TONMF ROC-curve. K=3, alpha = 1, beta = 0.5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display ℓ2 norm of columns of $Z$ outlier matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors_array = np.array(list(df_test['y_true'].values)).astype('str')\n",
    "colors_array[colors_array == '1'] = 'r'\n",
    "colors_array[colors_array != 'r'] = 'b'\n",
    "\n",
    "f = plt.figure(figsize=(20, 6))\n",
    "index = range(0, len(y_pred))\n",
    "plt.bar(index, y_pred, color = colors_array)\n",
    "f.savefig(\"Results of TONMF+L2 (outliers - red, non-outliers - blue).pdf\", bbox_inches='tight')\n",
    "plt.title(\"Results of TONMF+L2 (outliers - red, non-outliers - blue)\", size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [`rpca_implementation()`](https://alinapetukhova.github.io/textcl/docs/outliers_detection.html#textcl.outliers_detection.rpca_implementation) uses Robust Principal Component Analysis (RPCA) to obtain the outlier matrix. RPCA uses low-rank approximation and yields two matrices: low-rank matrix $L$ and a sparse matrix $S$. After normalization, the $S$ matrix represents the outlier score for the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_matrix = textcl.rpca_implementation(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize with l2-normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, y_pred = preprocessing.normalize(outlier_matrix, axis = 1, norm = 'l2', return_norm = True)\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(list(df_test['y_true'].values), y_pred, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "f.savefig(\"BBC dataset RPCA ROC-curve.pdf\", bbox_inches='tight')\n",
    "plt.title('BBC dataset RPCA ROC-curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display ℓ2 norm of columns of Z outlier matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors_array = np.array(list(df_test['y_true'].values)).astype('str')\n",
    "colors_array[colors_array == '1'] = 'r'\n",
    "colors_array[colors_array != 'r'] = 'b'\n",
    "\n",
    "f = plt.figure(figsize=(20, 6))\n",
    "plt.ylim(0.001, 0.00175)\n",
    "index = range(0, len(y_pred))\n",
    "plt.bar(index, y_pred, color = colors_array)\n",
    "f.savefig(\"Results of RPCA+L2 (outliers - red, non-outliers - blue).pdf\", bbox_inches='tight')\n",
    "plt.title(\"Results of RPCA+L2 (outliers - red, non-outliers - blue)\", size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [`svd()`](https://alinapetukhova.github.io/textcl/docs/outliers_detection.html#textcl.outliers_detection.svd) uses singular value decomposition (SVD) to obtain the outlier matrix. SVD is performed with the `np.linalg` function from the **numpy** package. The outlier matrix is presented as the multiplication of square root of diagonal elements of the rectangular diagonal matrix $S$ and complex unitary matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "outlier_matrix = textcl.svd(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize with l2-normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, y_pred = preprocessing.normalize(outlier_matrix, axis = 1, norm = 'l2', return_norm = True)\n",
    "\n",
    "f = plt.figure(figsize=(10, 8))\n",
    "fpr, tpr, thresholds = metrics.roc_curve(list(df_test['y_true'].values), y_pred, pos_label=1)\n",
    "plt.plot(fpr, tpr, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('BBC dataset SVD ROC-curve')\n",
    "f.savefig(\"BBC dataset SVD ROC-curve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display ℓ2 norm of columns of Z outlier matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "colors_array = np.array(list(df_test['y_true'].values)).astype('str')\n",
    "colors_array[colors_array == '1'] = 'r'\n",
    "colors_array[colors_array != 'r'] = 'b'\n",
    "\n",
    "f = plt.figure(figsize=(20, 6))\n",
    "# plt.ylim(0.001, 0.00175)\n",
    "index = range(0, len(y_pred))\n",
    "plt.bar(index, y_pred, color = colors_array)\n",
    "f.savefig(\"Results of SVD+L2 (outliers - red, non-outliers - blue).pdf\", bbox_inches='tight')\n",
    "plt.title(\"Results of SVD+L2 (outliers - red, non-outliers - blue)\", size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC-curve plot was used for the evaluation of the algorithm results. By analyzing obtained plots in these 3 examples we can see that with the current parameters for the BBC data set the **RPCA** method is offering the best results. In the plot \"Results of RPCA+L2 (outliers - red, non-outliers - blue)\" we can see the abnormal texts have higher values of the ℓ2 norm and can be segmented from the main distribution. AUC here represents the degree of separability between the main text group and outlied texts, the higher the AUC, the better the model is at distinguishing between different types of texts in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "ef6fb3f656d4ea79ca9f8b9c23b276d8478a1b3139acf0a655e7d2ebf2ef5905"
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
